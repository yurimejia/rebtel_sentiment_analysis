{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.book import *\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import nltk.data\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import tnt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c671f36e7ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "brown.words()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text1)\n",
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning is the science of getting computers to act without being explicitly programmed.',\n",
       " 'In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome.',\n",
       " 'Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it.',\n",
       " 'Many researchers also think it is the best way to make progress towards human-level AI.',\n",
       " 'In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.',\n",
       " \"More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems.\",\n",
       " \"Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)\n",
    "tokens = word_tokenize(text)\n",
    "tokens[:8]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tokens = pos_tag(tokens)\n",
    "tagged_tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this’s a sent tokenize test.',\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " 'Now it’s your turn.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "english_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.', 'Ahorita estoy comiendo, y tu?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien. Ahorita estoy comiendo, y tu?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this’s', 'a', 'test']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"this’s a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging and POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dive',\n",
       " 'into',\n",
       " 'NLTK',\n",
       " ':',\n",
       " 'Part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'POS',\n",
       " 'Tagger',\n",
       " 'apples']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"Dive into NLTK: Part-of-speech tagging and POS Tagger apples\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dive', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Part-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagger', 'NNP'),\n",
       " ('apples', 'NNS')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a POST tagging model or POS taggerin NLTK\n",
    "train_data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875545003237643"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the first 3000 treebank tagged sentences as the train_data, \n",
    "# last 914 tagged sentences as the test_data, \n",
    "# we train TnT POS Tagger by the train_data and evaluate it by the test_data:\n",
    "tnt_pos_tagger = tnt.TnT()\n",
    "tnt_pos_tagger.train(train_data)\n",
    "tnt_pos_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('tnt_treebank_pos_tagger.pickle', 'wb')\n",
    "pickle.dump(tnt_pos_tagger, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_tnt_tagger = pickle.load(open( 'tnt_treebank_pos_tagger.pickle', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875545003237643"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tnt_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('tnt', 'Unk'),\n",
       " ('treebank', 'Unk'),\n",
       " ('tnt', 'Unk'),\n",
       " ('tagger', 'Unk')]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnt_pos_tagger.tag(nltk.word_tokenize('this is a tnt treebank tnt tagger'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lemmatization & stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provis'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('saying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cri'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('crying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provid'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('crying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('churches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacus'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('abaci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hydrophobia'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('hydrophobia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.2'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI, MultiClassifierI\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "from nltk import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gender classification\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aamir', 'male'),\n",
       " ('Aaron', 'male'),\n",
       " ('Abbey', 'male'),\n",
       " ('Abbie', 'male'),\n",
       " ('Abbot', 'male'),\n",
       " ('Abbott', 'male'),\n",
       " ('Abby', 'male'),\n",
       " ('Abdel', 'male'),\n",
       " ('Abdul', 'male'),\n",
       " ('Abdulkarim', 'male'),\n",
       " ('Abdullah', 'male'),\n",
       " ('Abe', 'male'),\n",
       " ('Abel', 'male'),\n",
       " ('Abelard', 'male'),\n",
       " ('Abner', 'male')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namen = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "namen[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(namen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(namen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kamilah', 'female'),\n",
       " ('Carmelle', 'female'),\n",
       " ('Marvin', 'male'),\n",
       " ('Cheslie', 'female'),\n",
       " ('Jamey', 'male'),\n",
       " ('Godart', 'male'),\n",
       " ('Zach', 'male'),\n",
       " ('Dante', 'male'),\n",
       " ('Tandie', 'female'),\n",
       " ('Aubine', 'female')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namen[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'y'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Gary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': 'h'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 't'}, 'male'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), g) for (n, g) in namen]\n",
    "featuresets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7444, 500)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(gender_features('Gary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(gender_features('Mary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(gender_features('Dennis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(gender_features('Charles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(gender_features('Grace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(nb_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     34.4 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.8 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.0 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.6 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Maximum Entropy Classifier for Gender Identification:\n",
    "from nltk import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.37370        0.762\n",
      "             3          -0.37330        0.762\n",
      "             4          -0.37305        0.762\n",
      "             5          -0.37289        0.762\n",
      "             6          -0.37278        0.762\n",
      "             7          -0.37269        0.762\n",
      "             8          -0.37262        0.762\n",
      "             9          -0.37257        0.762\n",
      "            10          -0.37252        0.762\n",
      "            11          -0.37249        0.762\n",
      "            12          -0.37245        0.762\n",
      "            13          -0.37243        0.762\n",
      "            14          -0.37240        0.762\n",
      "            15          -0.37238        0.762\n",
      "            16          -0.37237        0.762\n",
      "            17          -0.37235        0.762\n",
      "            18          -0.37234        0.762\n",
      "            19          -0.37232        0.762\n",
      "            20          -0.37231        0.762\n",
      "            21          -0.37230        0.762\n",
      "            22          -0.37229        0.762\n",
      "            23          -0.37228        0.762\n",
      "            24          -0.37227        0.762\n",
      "            25          -0.37227        0.762\n",
      "            26          -0.37226        0.762\n",
      "            27          -0.37225        0.762\n",
      "            28          -0.37225        0.762\n",
      "            29          -0.37224        0.762\n",
      "            30          -0.37224        0.762\n",
      "            31          -0.37223        0.762\n",
      "            32          -0.37223        0.762\n",
      "            33          -0.37222        0.762\n",
      "            34          -0.37222        0.762\n",
      "            35          -0.37221        0.762\n",
      "            36          -0.37221        0.762\n",
      "            37          -0.37221        0.762\n",
      "            38          -0.37220        0.762\n",
      "            39          -0.37220        0.762\n",
      "            40          -0.37220        0.762\n",
      "            41          -0.37219        0.762\n",
      "            42          -0.37219        0.762\n",
      "            43          -0.37219        0.762\n",
      "            44          -0.37219        0.762\n",
      "            45          -0.37218        0.762\n",
      "            46          -0.37218        0.762\n",
      "            47          -0.37218        0.762\n",
      "            48          -0.37218        0.762\n",
      "            49          -0.37218        0.762\n",
      "            50          -0.37217        0.762\n",
      "            51          -0.37217        0.762\n",
      "            52          -0.37217        0.762\n",
      "            53          -0.37217        0.762\n",
      "            54          -0.37217        0.762\n",
      "            55          -0.37217        0.762\n",
      "            56          -0.37216        0.762\n",
      "            57          -0.37216        0.762\n",
      "            58          -0.37216        0.762\n",
      "            59          -0.37216        0.762\n",
      "            60          -0.37216        0.762\n",
      "            61          -0.37216        0.762\n",
      "            62          -0.37216        0.762\n",
      "            63          -0.37216        0.762\n",
      "            64          -0.37215        0.762\n",
      "            65          -0.37215        0.762\n",
      "            66          -0.37215        0.762\n",
      "            67          -0.37215        0.762\n",
      "            68          -0.37215        0.762\n",
      "            69          -0.37215        0.762\n",
      "            70          -0.37215        0.762\n",
      "            71          -0.37215        0.762\n",
      "            72          -0.37215        0.762\n",
      "            73          -0.37214        0.762\n",
      "            74          -0.37214        0.762\n",
      "            75          -0.37214        0.762\n",
      "            76          -0.37214        0.762\n",
      "            77          -0.37214        0.762\n",
      "            78          -0.37214        0.762\n",
      "            79          -0.37214        0.762\n",
      "            80          -0.37214        0.762\n",
      "            81          -0.37214        0.762\n",
      "            82          -0.37214        0.762\n",
      "            83          -0.37214        0.762\n",
      "            84          -0.37214        0.762\n",
      "            85          -0.37214        0.762\n",
      "            86          -0.37213        0.762\n",
      "            87          -0.37213        0.762\n",
      "            88          -0.37213        0.762\n",
      "            89          -0.37213        0.762\n",
      "            90          -0.37213        0.762\n",
      "            91          -0.37213        0.762\n",
      "            92          -0.37213        0.762\n",
      "            93          -0.37213        0.762\n",
      "            94          -0.37213        0.762\n",
      "            95          -0.37213        0.762\n",
      "            96          -0.37213        0.762\n",
      "            97          -0.37213        0.762\n",
      "            98          -0.37213        0.762\n",
      "            99          -0.37213        0.762\n",
      "         Final          -0.37213        0.762\n"
     ]
    }
   ],
   "source": [
    "me_classifier = MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_classifier.classify(gender_features('Gary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_classifier.classify(gender_features('Grace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_classifier.classify(gender_features('Yuri'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(me_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.644 last_letter==' ' and label is 'female'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "  -4.920 last_letter=='a' and label is 'male'\n",
      "  -3.503 last_letter=='k' and label is 'female'\n",
      "  -2.644 last_letter=='f' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count(a)': 3,\n",
       " 'count(b)': 0,\n",
       " 'count(c)': 0,\n",
       " 'count(d)': 1,\n",
       " 'count(e)': 1,\n",
       " 'count(f)': 0,\n",
       " 'count(g)': 0,\n",
       " 'count(h)': 0,\n",
       " 'count(i)': 3,\n",
       " 'count(j)': 1,\n",
       " 'count(k)': 0,\n",
       " 'count(l)': 0,\n",
       " 'count(m)': 2,\n",
       " 'count(n)': 1,\n",
       " 'count(o)': 0,\n",
       " 'count(p)': 0,\n",
       " 'count(q)': 0,\n",
       " 'count(r)': 2,\n",
       " 'count(s)': 0,\n",
       " 'count(t)': 0,\n",
       " 'count(u)': 1,\n",
       " 'count(v)': 0,\n",
       " 'count(w)': 0,\n",
       " 'count(x)': 0,\n",
       " 'count(y)': 1,\n",
       " 'count(z)': 0,\n",
       " 'firstletter': 'y',\n",
       " 'has(a)': True,\n",
       " 'has(b)': False,\n",
       " 'has(c)': False,\n",
       " 'has(d)': True,\n",
       " 'has(e)': True,\n",
       " 'has(f)': False,\n",
       " 'has(g)': False,\n",
       " 'has(h)': False,\n",
       " 'has(i)': True,\n",
       " 'has(j)': True,\n",
       " 'has(k)': False,\n",
       " 'has(l)': False,\n",
       " 'has(m)': True,\n",
       " 'has(n)': True,\n",
       " 'has(o)': False,\n",
       " 'has(p)': False,\n",
       " 'has(q)': False,\n",
       " 'has(r)': True,\n",
       " 'has(s)': False,\n",
       " 'has(t)': False,\n",
       " 'has(u)': True,\n",
       " 'has(v)': False,\n",
       " 'has(w)': False,\n",
       " 'has(x)': False,\n",
       " 'has(y)': True,\n",
       " 'has(z)': False,\n",
       " 'lastletter': 'a'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features2('Yuri Mejia Miranda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(gender_features2(n), g) for (n, g) in namen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb2_classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(nb2_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(nb_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.61161        0.630\n",
      "             3          -0.59771        0.630\n",
      "             4          -0.58460        0.630\n",
      "             5          -0.57227        0.636\n",
      "             6          -0.56069        0.649\n",
      "             7          -0.54981        0.670\n",
      "             8          -0.53960        0.685\n",
      "             9          -0.53001        0.701\n",
      "            10          -0.52101        0.714\n",
      "            11          -0.51255        0.725\n",
      "            12          -0.50460        0.734\n",
      "            13          -0.49711        0.743\n",
      "            14          -0.49006        0.749\n",
      "            15          -0.48342        0.755\n",
      "            16          -0.47715        0.758\n",
      "            17          -0.47122        0.763\n",
      "            18          -0.46562        0.766\n",
      "            19          -0.46031        0.769\n",
      "            20          -0.45529        0.771\n",
      "            21          -0.45052        0.774\n",
      "            22          -0.44600        0.775\n",
      "            23          -0.44169        0.777\n",
      "            24          -0.43760        0.778\n",
      "            25          -0.43371        0.779\n",
      "            26          -0.43000        0.780\n",
      "            27          -0.42645        0.782\n",
      "            28          -0.42308        0.783\n",
      "            29          -0.41985        0.783\n",
      "            30          -0.41676        0.783\n",
      "            31          -0.41381        0.785\n",
      "            32          -0.41098        0.786\n",
      "            33          -0.40827        0.786\n",
      "            34          -0.40568        0.787\n",
      "            35          -0.40318        0.788\n",
      "            36          -0.40079        0.788\n",
      "            37          -0.39850        0.789\n",
      "            38          -0.39629        0.789\n",
      "            39          -0.39416        0.790\n",
      "            40          -0.39212        0.791\n",
      "            41          -0.39015        0.792\n",
      "            42          -0.38826        0.792\n",
      "            43          -0.38643        0.792\n",
      "            44          -0.38467        0.793\n",
      "            45          -0.38297        0.795\n",
      "            46          -0.38134        0.795\n",
      "            47          -0.37975        0.795\n",
      "            48          -0.37822        0.795\n",
      "            49          -0.37675        0.796\n",
      "            50          -0.37532        0.796\n",
      "            51          -0.37394        0.796\n",
      "            52          -0.37260        0.796\n",
      "            53          -0.37131        0.797\n",
      "            54          -0.37005        0.797\n",
      "            55          -0.36884        0.797\n",
      "            56          -0.36766        0.797\n",
      "            57          -0.36652        0.797\n",
      "            58          -0.36542        0.798\n",
      "            59          -0.36435        0.798\n",
      "            60          -0.36330        0.798\n",
      "            61          -0.36229        0.799\n",
      "            62          -0.36131        0.798\n",
      "            63          -0.36036        0.798\n",
      "            64          -0.35944        0.798\n",
      "            65          -0.35854        0.798\n",
      "            66          -0.35766        0.799\n",
      "            67          -0.35681        0.799\n",
      "            68          -0.35598        0.799\n",
      "            69          -0.35518        0.799\n",
      "            70          -0.35440        0.799\n",
      "            71          -0.35363        0.800\n",
      "            72          -0.35289        0.799\n",
      "            73          -0.35217        0.799\n",
      "            74          -0.35146        0.799\n",
      "            75          -0.35078        0.799\n",
      "            76          -0.35011        0.800\n",
      "            77          -0.34946        0.800\n",
      "            78          -0.34882        0.800\n",
      "            79          -0.34820        0.800\n",
      "            80          -0.34760        0.800\n",
      "            81          -0.34701        0.800\n",
      "            82          -0.34643        0.800\n",
      "            83          -0.34587        0.801\n",
      "            84          -0.34532        0.801\n",
      "            85          -0.34478        0.801\n",
      "            86          -0.34426        0.801\n",
      "            87          -0.34375        0.801\n",
      "            88          -0.34325        0.801\n",
      "            89          -0.34276        0.801\n",
      "            90          -0.34228        0.801\n",
      "            91          -0.34182        0.801\n",
      "            92          -0.34136        0.801\n",
      "            93          -0.34091        0.801\n",
      "            94          -0.34048        0.801\n",
      "            95          -0.34005        0.802\n",
      "            96          -0.33963        0.802\n",
      "            97          -0.33922        0.802\n",
      "            98          -0.33882        0.802\n",
      "            99          -0.33843        0.802\n",
      "         Final          -0.33805        0.802\n"
     ]
    }
   ],
   "source": [
    "me2_classifier = MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(me2_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(me_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features3(name):\n",
    "    features = {}\n",
    "    features[\"fl\"] = name[0].lower()\n",
    "    features[\"ll\"] = name[-1].lower()\n",
    "    features[\"fw\"] = name[:2].lower()\n",
    "    features[\"lw\"] = name[-2:].lower()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fl': 'g', 'fw': 'ga', 'll': 'y', 'lw': 'ry'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('Gary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fl': 'y', 'fw': 'yu', 'll': 'i', 'lw': 'ri'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('Yuri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fl': 'g', 'fw': 'g', 'll': 'g', 'lw': 'g'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(gender_features3(n), g) for (n, g) in namen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fl': 'k', 'fw': 'ka', 'll': 'h', 'lw': 'ah'}, 'female')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb3_classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(nb3_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.40541        0.800\n",
      "             3          -0.34929        0.819\n",
      "             4          -0.32423        0.823\n",
      "             5          -0.31019        0.826\n",
      "             6          -0.30122        0.828\n",
      "             7          -0.29496        0.830\n",
      "             8          -0.29032        0.831\n",
      "             9          -0.28674        0.831\n",
      "            10          -0.28386        0.832\n",
      "            11          -0.28151        0.832\n",
      "            12          -0.27953        0.833\n",
      "            13          -0.27785        0.833\n",
      "            14          -0.27640        0.834\n",
      "            15          -0.27514        0.835\n",
      "            16          -0.27403        0.835\n",
      "            17          -0.27305        0.836\n",
      "            18          -0.27217        0.836\n",
      "            19          -0.27138        0.836\n",
      "            20          -0.27066        0.836\n",
      "            21          -0.27002        0.837\n",
      "            22          -0.26943        0.837\n",
      "            23          -0.26889        0.837\n",
      "            24          -0.26839        0.837\n",
      "            25          -0.26793        0.837\n",
      "            26          -0.26751        0.837\n",
      "            27          -0.26712        0.837\n",
      "            28          -0.26675        0.838\n",
      "            29          -0.26641        0.837\n",
      "            30          -0.26609        0.837\n",
      "            31          -0.26579        0.837\n",
      "            32          -0.26551        0.837\n",
      "            33          -0.26524        0.837\n",
      "            34          -0.26499        0.837\n",
      "            35          -0.26475        0.837\n",
      "            36          -0.26453        0.837\n",
      "            37          -0.26432        0.837\n",
      "            38          -0.26412        0.837\n",
      "            39          -0.26392        0.837\n",
      "            40          -0.26374        0.837\n",
      "            41          -0.26357        0.837\n",
      "            42          -0.26340        0.837\n",
      "            43          -0.26324        0.837\n",
      "            44          -0.26309        0.837\n",
      "            45          -0.26294        0.837\n",
      "            46          -0.26280        0.837\n",
      "            47          -0.26267        0.837\n",
      "            48          -0.26254        0.837\n",
      "            49          -0.26241        0.837\n",
      "            50          -0.26229        0.837\n",
      "            51          -0.26218        0.837\n",
      "            52          -0.26206        0.837\n",
      "            53          -0.26196        0.837\n",
      "            54          -0.26185        0.838\n",
      "            55          -0.26175        0.838\n",
      "            56          -0.26165        0.838\n",
      "            57          -0.26156        0.838\n",
      "            58          -0.26147        0.838\n",
      "            59          -0.26138        0.838\n",
      "            60          -0.26129        0.838\n",
      "            61          -0.26121        0.838\n",
      "            62          -0.26113        0.838\n",
      "            63          -0.26105        0.838\n",
      "            64          -0.26097        0.838\n",
      "            65          -0.26090        0.838\n",
      "            66          -0.26083        0.838\n",
      "            67          -0.26076        0.838\n",
      "            68          -0.26069        0.838\n",
      "            69          -0.26062        0.838\n",
      "            70          -0.26056        0.838\n",
      "            71          -0.26050        0.838\n",
      "            72          -0.26044        0.838\n",
      "            73          -0.26038        0.838\n",
      "            74          -0.26032        0.838\n",
      "            75          -0.26026        0.838\n",
      "            76          -0.26020        0.838\n",
      "            77          -0.26015        0.838\n",
      "            78          -0.26010        0.838\n",
      "            79          -0.26005        0.838\n",
      "            80          -0.26000        0.838\n",
      "            81          -0.25995        0.838\n",
      "            82          -0.25990        0.838\n",
      "            83          -0.25985        0.838\n",
      "            84          -0.25980        0.838\n",
      "            85          -0.25976        0.838\n",
      "            86          -0.25971        0.838\n",
      "            87          -0.25967        0.838\n",
      "            88          -0.25963        0.838\n",
      "            89          -0.25959        0.838\n",
      "            90          -0.25955        0.838\n",
      "            91          -0.25951        0.838\n",
      "            92          -0.25947        0.838\n",
      "            93          -0.25943        0.838\n",
      "            94          -0.25939        0.838\n",
      "            95          -0.25936        0.838\n",
      "            96          -0.25932        0.838\n",
      "            97          -0.25929        0.838\n",
      "            98          -0.25925        0.838\n",
      "            99          -0.25922        0.838\n",
      "         Final          -0.25918        0.838\n"
     ]
    }
   ],
   "source": [
    "me3_classifier = MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(me3_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Mary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Yuri'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Aldo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Dennis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Yuridia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3_classifier.classify(gender_features3('Charlotte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum entropy modeling = Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides several learning algorithms for text classification, such as naive bayes, decision trees, and also includes maximum entropy models, you can find them all in the nltk/classify module. For Maximum entropy modeling, you can find the details in the maxent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category) \\\n",
    "             for category in movie_reviews.categories() \\\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents[1];random.documents[1];shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total number of movie reviews documents in nltk is 2000\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct a list of the 2,000 most frequent words in the overall corpus \n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words['gertz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a feature extractor that simply checks whether each of these words is present in a given document.\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_features(movie_reviews.words('pos/cv957_8737.txt'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the feature sets for the movie review documents one by one\n",
    "featuresets = [(document_features(d), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the train set (1900 documents) and test set (100 documents)\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a naive bayes classifier with train set by nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the accuracy of the naive bayes classifier with test set\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   contains(fascination) = True              pos : neg    =     10.2 : 1.0\n",
      "  contains(manipulation) = True              pos : neg    =     10.2 : 1.0\n",
      "          contains(sans) = True              neg : pos    =      9.1 : 1.0\n",
      "       contains(frances) = True              pos : neg    =      8.9 : 1.0\n",
      "         contains(sweep) = True              pos : neg    =      6.9 : 1.0\n",
      "   contains(magnificent) = True              pos : neg    =      6.7 : 1.0\n",
      "        contains(regard) = True              pos : neg    =      6.5 : 1.0\n",
      "       contains(misfire) = True              neg : pos    =      6.4 : 1.0\n",
      "       contains(cunning) = True              pos : neg    =      6.3 : 1.0\n",
      "  contains(breathtaking) = True              pos : neg    =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Debug info: show top n most informative features\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the top-2000 word features, we can train a Maximum entropy classifier model with NLTK and MEGAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1310: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1312: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1313: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1320: RuntimeWarning: invalid value encountered in true_divide\n",
      "  deltas -= (ffreq_empirical - sum1) / -sum2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.497\n"
     ]
    }
   ],
   "source": [
    "maxent_classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = \"I love this movie, very interesting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'movie,', 'very', 'interesting']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = document_features(test_text.split())\n",
    "test_set;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naivebayes classifier get the wrong result\n",
    "classifier.classify(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see the probability result\n",
    "prob_result = classifier.prob_classify(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_result.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627510883868112"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_result.prob(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13724891161318822"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_result.prob(\"pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = [(bag_of_words(d), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = data_sets[100:], data_sets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(bayes_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     12.8 : 1.0\n",
      "               affecting = True              pos : neg    =     11.5 : 1.0\n",
      "              astounding = True              pos : neg    =     11.5 : 1.0\n",
      "                    slip = True              pos : neg    =     11.5 : 1.0\n",
      "                    3000 = True              neg : pos    =     11.1 : 1.0\n",
      "                 insipid = True              neg : pos    =     11.1 : 1.0\n",
      "               insulting = True              neg : pos    =     11.1 : 1.0\n",
      "             outstanding = True              pos : neg    =     10.7 : 1.0\n",
      "                headache = True              neg : pos    =     10.5 : 1.0\n",
      "               atrocious = True              neg : pos    =     10.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "bayes_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1310: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1312: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1313: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n",
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/nltk/classify/maxent.py:1320: RuntimeWarning: invalid value encountered in true_divide\n",
      "  deltas -= (ffreq_empirical - sum1) / -sum2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.69315        0.503\n"
     ]
    }
   ],
   "source": [
    "maxent_bg_classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the bigrams feature in the classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_ngrams(words, n=2):\n",
    "    ngs = [ng for ng in iter(ngrams(words, n))]\n",
    "    return bag_of_words(ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = [(bag_of_ngrams(d), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = data_sets[100:], data_sets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_bi_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nb_bi_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        ('not', 'funny') = True              neg : pos    =     17.2 : 1.0\n",
      "      ('is', 'terrific') = True              pos : neg    =     16.8 : 1.0\n",
      "       ('bad', 'acting') = True              neg : pos    =     13.2 : 1.0\n",
      "          ('awful', '.') = True              neg : pos    =     13.2 : 1.0\n",
      "       ('and', 'boring') = True              neg : pos    =     13.2 : 1.0\n",
      "        ('.', 'cameron') = True              pos : neg    =     12.8 : 1.0\n",
      "        ('insult', 'to') = True              neg : pos    =     12.5 : 1.0\n",
      "          ('our', 'own') = True              pos : neg    =     12.2 : 1.0\n",
      "       ('works', 'well') = True              pos : neg    =     12.2 : 1.0\n",
      "    ('quite', 'frankly') = True              neg : pos    =     11.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_bi_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_all(words, n=2):\n",
    "    all_features = bag_of_words(words)\n",
    "    ngram_features = bag_of_ngrams(words, n=n)\n",
    "    all_features.update(ngram_features)   \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = [(bag_of_all(d), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = data_sets[100:], data_sets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_all_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nb_all_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        ('not', 'funny') = True              neg : pos    =     17.2 : 1.0\n",
      "      ('is', 'terrific') = True              pos : neg    =     16.8 : 1.0\n",
      "       ('bad', 'acting') = True              neg : pos    =     13.2 : 1.0\n",
      "          ('awful', '.') = True              neg : pos    =     13.2 : 1.0\n",
      "       ('and', 'boring') = True              neg : pos    =     13.2 : 1.0\n",
      "        ('.', 'cameron') = True              pos : neg    =     12.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     12.8 : 1.0\n",
      "        ('insult', 'to') = True              neg : pos    =     12.5 : 1.0\n",
      "          ('our', 'own') = True              pos : neg    =     12.2 : 1.0\n",
      "       ('works', 'well') = True              pos : neg    =     12.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_all_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive Into NLTK, Part X: Play with Word2Vec Models based on NLTK Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import logging\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "austen_emma_words = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_emma_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austen_emma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "austen_emma_sents = gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "austen_emma_words_nopunct = [[word.lower() for word in sent \\\n",
    "                              if word not in punctuation] \\\n",
    "                             for sent in austen_emma_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7752"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austen_emma_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'],\n",
       " ['VOLUME', 'I'],\n",
       " ['CHAPTER', 'I']]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_emma_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_emma_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'and',\n",
       " 'Mrs',\n",
       " '.',\n",
       " 'Weston',\n",
       " 'were',\n",
       " 'both',\n",
       " 'dreadfully',\n",
       " 'desponding',\n",
       " '.']"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_emma_sents[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'austen_emma_words_nopunct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-a68f2e5e36a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mausten_emma_words_nopunct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'austen_emma_words_nopunct' is not defined"
     ]
    }
   ],
   "source": [
    "austen_emma_words_nopunct[3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [kpn]",
   "language": "python",
   "name": "Python [kpn]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
