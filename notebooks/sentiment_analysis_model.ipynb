{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS USING REVIEW'S STARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code from http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "import nltk.classify.util\n",
    "import collections, itertools\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import scores, sent_tokenize, word_tokenize, pos_tag, MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO EVALUATE MODELS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate features\n",
    "def evaluate_classifier(featx,                         \n",
    "                        negtrain,\n",
    "                        negtest,\n",
    "                        postrain, \n",
    "                        postest):     \n",
    "    \n",
    "    negtrain_feats = [(featx(w), 'neg') for w in negtrain]\n",
    "    negtest_feats  = [(featx(w), 'neg') for w in negtest]\n",
    "    postrain_feats = [(featx(w), 'pos') for w in postrain]\n",
    "    postest_feats  = [(featx(w), 'pos') for w in postest]\n",
    "    \n",
    "    trainfeats = negtrain_feats + postrain_feats\n",
    "    testfeats = negtest_feats + postest_feats    \n",
    "    trainfeats = random.sample(trainfeats, len(trainfeats))\n",
    "    testfeats = random.sample(testfeats, len(testfeats))    \n",
    "        \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    # classifier = MaxentClassifier.train(trainfeats)\n",
    "    \n",
    "    actual = []\n",
    "    predict = []\n",
    "\n",
    "    for i, (feats, label) in enumerate(testfeats):        \n",
    "            observed = classifier.classify(feats)\n",
    "            actual.append(label)\n",
    "            predict.append(observed)\n",
    "    try:\n",
    "        accuracy = accuracy_score(actual, predict)\n",
    "    except:\n",
    "        accuracy = None\n",
    "        print('Accuracy: Division by zero')\n",
    "\n",
    "    try:\n",
    "        precision = precision_score(actual, predict)\n",
    "    except:\n",
    "        precision = None\n",
    "        print('Precision: Division by zero')\n",
    "\n",
    "    try:\n",
    "        recall = recall_score(actual, predict)\n",
    "    except:\n",
    "        recall  = None\n",
    "        print('Recall: Division by zero')\n",
    "\n",
    "    try:\n",
    "        matthew = matthews_corrcoef(actual, predict)\n",
    "    except:\n",
    "        matthew = None\n",
    "        print('Matthews corr coeff: Division by zero')\n",
    "\n",
    "    return accuracy, precision, recall, matthew, classifier\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate features (include bestwords)\n",
    "def evaluate_classifier2(featx,                         \n",
    "                        negtrain,\n",
    "                        negtest,\n",
    "                        postrain, \n",
    "                        postest,\n",
    "                        bestwords):    \n",
    "    \n",
    "    negtrain_feats = [(featx(w, bestwords), 'neg') for w in negtrain]\n",
    "    negtest_feats  = [(featx(w, bestwords), 'neg') for w in negtest]\n",
    "    postrain_feats = [(featx(w, bestwords), 'pos') for w in postrain]\n",
    "    postest_feats  = [(featx(w, bestwords), 'pos') for w in postest]\n",
    "    \n",
    "    trainfeats = negtrain_feats + postrain_feats\n",
    "    testfeats = negtest_feats + postest_feats    \n",
    "    trainfeats = random.sample(trainfeats, len(trainfeats))\n",
    "    testfeats = random.sample(testfeats, len(testfeats))\n",
    "        \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    # classifier = MaxentClassifier.train(trainfeats)\n",
    "    \n",
    "    actual = []\n",
    "    predict = []\n",
    "\n",
    "    for i, (feats, label) in enumerate(testfeats):        \n",
    "            observed = classifier.classify(feats)\n",
    "            actual.append(label)\n",
    "            predict.append(observed)\n",
    "    try:\n",
    "        accuracy = accuracy_score(actual, predict)\n",
    "    except:\n",
    "        accuracy = None\n",
    "        print('Accuracy: Division by zero')\n",
    "\n",
    "    try:\n",
    "        precision = precision_score(actual, predict)\n",
    "    except:\n",
    "        precision = None\n",
    "        print('Precision: Division by zero')\n",
    "\n",
    "    try:\n",
    "        recall = recall_score(actual, predict)\n",
    "    except:\n",
    "        recall  = None\n",
    "        print('Recall: Division by zero')\n",
    "\n",
    "    try:\n",
    "        matthew = matthews_corrcoef(actual, predict)\n",
    "    except:\n",
    "        matthew = None\n",
    "        print('Matthews corr coeff: Division by zero')\n",
    "\n",
    "    return accuracy, precision, recall, matthew, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of words: All words\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stopword filtering\n",
    "stop_set = set(stopwords.words('english')) \n",
    "\n",
    "def stopword_filtered_word_feats(words, stopset = stop_set):\n",
    "    return dict([(word, True) for word in words if word not in stopset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bigram Collocations\n",
    "def bigram_word_feats(words, score_fn = BigramAssocMeasures.chi_sq, n = 200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eliminate Low Information Features\n",
    "def get_best_words(neg_train, pos_train, best_n):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "    for w in [word for review in neg_train for word in review]:\n",
    "        word_fd[w.lower()] += 1\n",
    "        label_word_fd['neg'][w.lower()] += 1\n",
    "\n",
    "    for w in [word for review in pos_train for word in review]:\n",
    "        word_fd[w.lower()] += 1\n",
    "        label_word_fd['pos'][w.lower()] += 1\n",
    "\n",
    "    neg_word_count = label_word_fd['neg'].N()\n",
    "    pos_word_count = label_word_fd['pos'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "    \n",
    "    # Compute scores\n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq( \\\n",
    "            label_word_fd['pos'][word], \\\n",
    "            (freq, pos_word_count), \\\n",
    "            total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq( \\\n",
    "            label_word_fd['neg'][word], \\\n",
    "            (freq, neg_word_count), \\\n",
    "            total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "    \n",
    "    # Choose best score\n",
    "    best = sorted(word_scores.items(), \\\n",
    "                  key=lambda s: s[1], \\\n",
    "                  reverse=True)[:best_n]    \n",
    "    return set([w for w, s in best])\n",
    "\n",
    "# Features (words) based on best words\n",
    "def best_word_feats(words, bestwords):\n",
    "    return dict([(word, True) for word in words if word in bestwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Best words + bigram\n",
    "def best_bigram_word_feats(words, \n",
    "                           bestwords,\n",
    "                           score_fn = BigramAssocMeasures.chi_sq, \n",
    "                           n = 200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    d = dict([(bigram, True) for bigram in bigrams])\n",
    "    d.update(best_word_feats(words, bestwords))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVELOP MODEL ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "file_name = '../data/rawdata_20170620.json'\n",
    "with open(file_name) as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(punctuation)])\n",
    "    return s\n",
    "\n",
    "rem = string.punctuation\n",
    "pattern = r\"[{}]\".format(rem)\n",
    "df['text_nopunct'] = df['text'].str.replace(pattern, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((471, 9), (1522, 9), (226, 9))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify reviews: negative vs positive vs neutral\n",
    "# Assumptions:\n",
    "# Reviews with 1,2 stars -> negative\n",
    "# Reviews with 4,5 stars -> positive\n",
    "# Revjiews with 3 stars -> neutral\n",
    "\n",
    "df_neg = df[(df.stars == 1) | (df.stars == 2)]\n",
    "df_pos = df[(df.stars == 4) | (df.stars == 5)]\n",
    "df_neu = df[(df.stars == 3)]\n",
    "df_neg.shape, df_pos.shape, df_neu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: We focused on positive and negative reviews (discard neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split reviews: positive vs negative\n",
    "neg_words = [word_tokenize(f) for f in df_neg.text_nopunct]\n",
    "pos_words = [word_tokenize(f) for f in df_pos.text_nopunct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data set: developing vs validation\n",
    "neg_develop, neg_val = train_test_split(neg_words, test_size=0.25)\n",
    "pos_develop, pos_val = train_test_split(pos_words, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 118, 1141, 381)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_develop),len(neg_val), len(pos_develop),len(pos_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-CROSS VALIDATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "k = 5\n",
    "\n",
    "results = collections.defaultdict(dict)\n",
    "models = ['bag_of_words', 'stop_words', 'bigram', \\\n",
    "          'best_words', 'bigram_best_words']\n",
    "for model_name in models:\n",
    "    results[model_name]['accuracy'] = []\n",
    "    results[model_name]['precision'] = []\n",
    "    results[model_name]['recall'] = []\n",
    "    results[model_name]['matthew'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Train on 1194 instances, test on 300 instances\n",
      "BAG_OF_WORDS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/sklearn/metrics/classification.py:1008: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if pos_label not in present_labels:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "BEST_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "1. Train on 1194 instances, test on 300 instances\n",
      "BAG_OF_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "BEST_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "2. Train on 1194 instances, test on 300 instances\n",
      "BAG_OF_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "BEST_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "3. Train on 1194 instances, test on 300 instances\n",
      "BAG_OF_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "BEST_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "4. Train on 1194 instances, test on 300 instances\n",
      "BAG_OF_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n",
      "BEST_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, k):\n",
    "    # Split developing dataset: training vs testing\n",
    "    neg_train, neg_test = train_test_split(neg_develop, test_size=1/k)\n",
    "    pos_train, pos_test = train_test_split(pos_develop, test_size=1/k)    \n",
    "        \n",
    "    num_train = len(neg_train) + len(pos_train)\n",
    "    num_test = len(neg_test) + len(pos_test)    \n",
    "    print(str(i) + '. Train on %d instances, test on %d instances' % \\\n",
    "          (num_train, num_test)) \n",
    "    \n",
    "    model_name = 'bag_of_words'\n",
    "    print(model_name.upper())    \n",
    "    accuracy, precision, recall, matthew, _ = evaluate_classifier(\n",
    "        word_feats,                         \n",
    "        negtrain = neg_train,\n",
    "        negtest = neg_test,\n",
    "        postrain = pos_train, \n",
    "        postest = pos_test) \n",
    "    results[model_name]['accuracy'].append(accuracy)\n",
    "    results[model_name]['precision'].append(precision)\n",
    "    results[model_name]['recall'].append(recall)\n",
    "    results[model_name]['matthew'].append(matthew)\n",
    "    \n",
    "        \n",
    "    model_name = 'stop_words'\n",
    "    print(model_name.upper())     \n",
    "    accuracy, precision, recall, matthew, _ = evaluate_classifier(\n",
    "        stopword_filtered_word_feats,                         \n",
    "        negtrain = neg_train,\n",
    "        negtest = neg_test,\n",
    "        postrain = pos_train, \n",
    "        postest = pos_test)\n",
    "    results[model_name]['accuracy'].append(accuracy)\n",
    "    results[model_name]['precision'].append(precision)\n",
    "    results[model_name]['recall'].append(recall)\n",
    "    results[model_name]['matthew'].append(matthew)\n",
    "    \n",
    "#     model_name = 'bigram'\n",
    "#     print(model_name.upper())     \n",
    "#     accuracy, precision, recall, matthew, _ = evaluate_classifier(\n",
    "#         bigram_word_feats,                         \n",
    "#         negtrain = neg_train,\n",
    "#         negtest = neg_test,\n",
    "#         postrain = pos_train, \n",
    "#         postest = pos_test) \n",
    "#     results[model_name]['accuracy'].append(accuracy)\n",
    "#     results[model_name]['precision'].append(precision)\n",
    "#     results[model_name]['recall'].append(recall)\n",
    "#     results[model_name]['matthew'].append(matthew)\n",
    "    \n",
    "    best_words = get_best_words(neg_train, pos_train, 5000)                         \n",
    "    model_name = 'best_words'    \n",
    "    print(model_name.upper())            \n",
    "    accuracy, precision, recall, matthew, _ = evaluate_classifier2(\n",
    "        best_word_feats,                        \n",
    "        negtrain = neg_train,\n",
    "        negtest = neg_test,\n",
    "        postrain = pos_train, \n",
    "        postest = pos_test,\n",
    "        bestwords = best_words) \n",
    "    results[model_name]['accuracy'].append(accuracy)\n",
    "    results[model_name]['precision'].append(precision)\n",
    "    results[model_name]['recall'].append(recall)\n",
    "    results[model_name]['matthew'].append(matthew)\n",
    "                     \n",
    "#     model_name = 'bigram_best_words'\n",
    "#     print(model_name.upper())         \n",
    "#     accuracy, precision, recall, matthew, _ = evaluate_classifier2(\n",
    "#         best_bigram_word_feats,                         \n",
    "#         negtrain = neg_train,\n",
    "#         negtest = neg_test,\n",
    "#         postrain = pos_train, \n",
    "#         postest = pos_test,\n",
    "#         bestwords = best_words)\n",
    "#     results[model_name]['accuracy'].append(accuracy)\n",
    "#     results[model_name]['precision'].append(precision)\n",
    "#     results[model_name]['recall'].append(recall)\n",
    "#     results[model_name]['matthew'].append(matthew)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matthew</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bag_of_words</th>\n",
       "      <td>0.229103</td>\n",
       "      <td>0.394667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_words</th>\n",
       "      <td>0.212495</td>\n",
       "      <td>0.398667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_words</th>\n",
       "      <td>0.314418</td>\n",
       "      <td>0.500667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               matthew  accuracy\n",
       "bag_of_words  0.229103  0.394667\n",
       "best_words    0.212495  0.398667\n",
       "stop_words    0.314418  0.500667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose Best Features\n",
    "best_features = collections.defaultdict(dict)\n",
    "for model_name in models:\n",
    "    a = [x for x in results[model_name]['accuracy'] if x is not None]\n",
    "    if len(a)>0:\n",
    "        best_features[model_name]['accuracy'] = mean(a)\n",
    "        \n",
    "    p = [x for x in results[model_name]['precision'] if x is not None]\n",
    "    if len(p)>0:\n",
    "        best_features[model_name]['precision'] = mean(p)\n",
    "        \n",
    "    r = [x for x in results[model_name]['recall'] if x is not None]\n",
    "    if len(r)>0:\n",
    "        best_features[model_name]['recall'] = mean(r)\n",
    "        \n",
    "    m = [x for x in results[model_name]['matthew'] if x is not None]\n",
    "    if len(m)>0:\n",
    "        best_features[model_name]['matthew'] = mean(m)     \n",
    "    \n",
    "# Highest accuracy, precision, recall, Matthews correlation coefficient\n",
    "pd.DataFrame.from_dict({(i): best_features[i]  \n",
    "                        for i in best_features.keys()}, \n",
    "                       orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MODEL: TRAIN ON FULL DEVELOPING DATA SET, EVALUATE ON VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1494 instances, test on 499 instances\n"
     ]
    }
   ],
   "source": [
    "num_train = len(neg_develop) + len(pos_develop)\n",
    "num_test = len(neg_val) + len(pos_val)    \n",
    "print('Train on %d instances, test on %d instances' % (num_train, num_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get best words\n",
    "best_words = get_best_words(neg_develop, pos_develop, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS\n",
      "Precision: Division by zero\n",
      "Recall: Division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda2/envs/kpn/lib/python3.4/site-packages/sklearn/metrics/classification.py:1008: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if pos_label not in present_labels:\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_name = 'stop_words'\n",
    "print(model_name.upper()) \n",
    "\n",
    "accuracy, precision, recall, matthew, best_classifier = evaluate_classifier(\n",
    "    stopword_filtered_word_feats,                         \n",
    "    negtrain = neg_develop,\n",
    "    negtest = neg_val,\n",
    "    postrain = pos_develop, \n",
    "    postest = pos_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.517034\n",
      "Matthew corr coeff: 0.333625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "if accuracy is not None: print('Accuracy: %f' % accuracy)\n",
    "if precision is not None: print('Precision: %f' % precision)\n",
    "if recall is not None: print('Recall: %f' % recall)\n",
    "if matthew is not None: print('Matthew corr coeff: %f' % matthew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  saying = True              neg : pos    =     50.5 : 1.0\n",
      "                    days = True              neg : pos    =     48.4 : 1.0\n",
      "                  refund = True              neg : pos    =     35.5 : 1.0\n",
      "                  asking = True              neg : pos    =     33.3 : 1.0\n",
      "                  cancel = True              neg : pos    =     33.3 : 1.0\n",
      "                  unable = True              neg : pos    =     31.2 : 1.0\n",
      "                   worst = True              neg : pos    =     30.3 : 1.0\n",
      "                      00 = True              neg : pos    =     29.0 : 1.0\n",
      "                 anymore = True              neg : pos    =     29.0 : 1.0\n",
      "                received = True              neg : pos    =     29.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Most relevant features\n",
    "best_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SAVE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('classify_reviews.pickle', 'wb')\n",
    "    pickle.dump(best_classifier, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [kpn]",
   "language": "python",
   "name": "Python [kpn]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
